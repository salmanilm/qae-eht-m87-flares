{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6b31204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "QUANTUM AUTOENCODER SETUP (PennyLane-Torch Hybrid)\n",
      "============================================================\n",
      "PennyLane version: 0.43.2\n",
      "TorchLayer available: True\n"
     ]
    }
   ],
   "source": [
    "# PennyLane-Torch Hybrid\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "\n",
    "# Pennylane's torch layer is a bit different so it is being used instead of pytorch\n",
    "from pennylane.qnn import TorchLayer\n",
    "\n",
    "# Add project root\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.data_loader import create_dataloaders\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"QUANTUM AUTOENCODER SETUP (PennyLane-Torch Hybrid)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PennyLane version: {qml.__version__}\")\n",
    "print(f\"TorchLayer available: {hasattr(qml.qnn, 'TorchLayer')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "155e1313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[M87FlareDataset] Loaded 1000 train samples\n",
      "[M87FlareDataset] Flare count: 0\n",
      "[M87FlareDataset] Loaded 300 val samples\n",
      "[M87FlareDataset] Flare count: 0\n",
      "[M87FlareDataset] Loaded 900 test samples\n",
      "[M87FlareDataset] Flare count: 200\n",
      "\n",
      "[SPLIT SUMMARY]\n",
      "Train: 1000 samples (normal only)\n",
      "Val:   300 samples (normal only)\n",
      "Test:  900 samples (normal + flare)\n",
      "Patch shape: torch.Size([1, 1, 4, 4])\n",
      "Encoded shape: (4096,), norm: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# amplitude encoding\n",
    "def amplitude_encode_12q(image_4x4):\n",
    "    \"\"\"4*4 patch → 12-qubit state (4096-dim)\"\"\"\n",
    "    if isinstance(image_4x4, torch.Tensor):\n",
    "        image_4x4 = image_4x4.cpu().numpy()\n",
    "    \n",
    "    if image_4x4.shape != (4, 4):\n",
    "        raise ValueError(f\"Expected (4,4), got {image_4x4.shape}\")\n",
    "    \n",
    "    flat = image_4x4.flatten().astype(np.float64)\n",
    "    padded = np.zeros(4096)\n",
    "    padded[:16] = flat\n",
    "    norm = np.linalg.norm(padded)\n",
    "    \n",
    "    if norm > 1e-10:\n",
    "        padded = padded / norm\n",
    "    \n",
    "    return padded\n",
    "\n",
    "# verify\n",
    "train_loader_test, _, _ = create_dataloaders(data_dir=\"../data/processed\", batch_size=1)\n",
    "img, _ = next(iter(train_loader_test))\n",
    "print(f\"Patch shape: {img.shape}\")\n",
    "encoded = amplitude_encode_12q(img[0, 0])\n",
    "print(f\"Encoded shape: {encoded.shape}, norm: {np.linalg.norm(encoded):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ec07d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchLayer created with weight shapes: {'weights': (3, 12, 3)}\n"
     ]
    }
   ],
   "source": [
    "# quantum circuit as TorchLayer (PennyLane)\n",
    "n_qubits = 12\n",
    "n_layers = 3\n",
    "\n",
    "# define device\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits, shots=None)\n",
    "\n",
    "# define quantum circuit that returns expectations\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_circuit(inputs, weights):\n",
    "    \"\"\"\n",
    "    inputs: NOT used (we'll encode manually)\n",
    "    weights: [n_layers, n_qubits, 3] quantum parameters\n",
    "    \"\"\"\n",
    "    # encode the image (passed via closure, not inputs)\n",
    "    # handled in the forward pass\n",
    "    \n",
    "    # variational layers\n",
    "    for layer_idx in range(n_layers):\n",
    "        layer_params = weights[layer_idx]\n",
    "        \n",
    "        # rotations\n",
    "        for i in range(n_qubits):\n",
    "            qml.RX(layer_params[i, 0], wires=i)\n",
    "            qml.RY(layer_params[i, 1], wires=i)\n",
    "            qml.RZ(layer_params[i, 2], wires=i)\n",
    "        \n",
    "        # entanglement (the cool part)\n",
    "        for i in range(n_qubits):\n",
    "            qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
    "    \n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "# create TorchLayer (should handle the quantum-classical interface automatically)\n",
    "weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
    "qlayer = TorchLayer(quantum_circuit, weight_shapes)\n",
    "\n",
    "print(f\"TorchLayer created with weight shapes: {weight_shapes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "532c2d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum parameters: 108\n",
      "Classical parameters: 944\n",
      "Total: 1052\n",
      "Input: torch.Size([2, 1, 4, 4]), Output: torch.Size([2, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Quantum-Classical Model\n",
    "class QuantumAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    PennyLane-Torch Hybrid Model\n",
    "    - Quantum: TorchLayer (handles gradients automatically)\n",
    "    - Classical: Standard PyTorch layers\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits=12, n_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_qubits = n_qubits\n",
    "        self.encoder = nn.Identity()  # encode manually\n",
    "        \n",
    "        # quantum layer (PennyLane should handle quantum parameters)\n",
    "        self.quantum_layer = qlayer\n",
    "        \n",
    "        # classical post-processing\n",
    "        self.scale_layer = nn.Sequential(\n",
    "            nn.Linear(n_qubits, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        print(f\"Quantum parameters: {sum(p.numel() for p in self.quantum_layer.parameters())}\")\n",
    "        print(f\"Classical parameters: {sum(p.numel() for p in self.scale_layer.parameters())}\")\n",
    "        print(f\"Total: {sum(p.numel() for p in self.parameters())}\")\n",
    "    \n",
    "    def forward(self, images_batch):\n",
    "        \"\"\"\n",
    "        Forward pass for batch of 4×4 images\n",
    "        \n",
    "        Args:\n",
    "            images_batch: torch.Tensor [batch, 1, 4, 4]\n",
    "        \n",
    "        Returns:\n",
    "            reconstructions: torch.Tensor [batch, 1, 4, 4]\n",
    "        \"\"\"\n",
    "        batch_size = images_batch.shape[0]\n",
    "        reconstructions = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # encode image to quantum state\n",
    "            img_np = images_batch[i, 0].cpu().numpy()\n",
    "            state = amplitude_encode_12q(img_np)\n",
    "            \n",
    "            # feed to quantum layer\n",
    "            # torchLayer expects inputs, but we don't use them (encoded via closure)\n",
    "            dummy_input = torch.zeros(1)  # dummy input (not used)\n",
    "            expectations = self.quantum_layer(dummy_input)  # quantum params are internal\n",
    "            \n",
    "            # classical post-processing\n",
    "            pixel_values = self.scale_layer(expectations)\n",
    "            reconstruction = pixel_values.reshape(1, 4, 4)\n",
    "            reconstructions.append(reconstruction)\n",
    "        \n",
    "        return torch.stack(reconstructions)\n",
    "    \n",
    "    def get_reconstruction_error(self, images_batch):\n",
    "        reconstructed = self.forward(images_batch)\n",
    "        mse = nn.functional.mse_loss(reconstructed, images_batch, reduction='none')\n",
    "        return mse.mean(dim=(1, 2, 3))\n",
    "\n",
    "# test\n",
    "model_q = QuantumAutoencoder()\n",
    "dummy_batch = torch.randn(2, 1, 4, 4)\n",
    "output = model_q(dummy_batch)\n",
    "print(f\"Input: {dummy_batch.shape}, Output: {output.shape}\")\n",
    "assert dummy_batch.shape == output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7bf1ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUANTUM TRAINING CONFIG\n",
      "============================================================\n",
      "  n_qubits: 12\n",
      "  n_layers: 3\n",
      "  batch_size: 16\n",
      "  lr: 0.01\n",
      "  epochs: 5\n",
      "  data_dir: ../data/processed\n",
      "[M87FlareDataset] Loaded 1000 train samples\n",
      "[M87FlareDataset] Flare count: 0\n",
      "[M87FlareDataset] Loaded 300 val samples\n",
      "[M87FlareDataset] Flare count: 0\n",
      "[M87FlareDataset] Loaded 900 test samples\n",
      "[M87FlareDataset] Flare count: 200\n",
      "\n",
      "[SPLIT SUMMARY]\n",
      "Train: 1000 samples (normal only)\n",
      "Val:   300 samples (normal only)\n",
      "Test:  900 samples (normal + flare)\n",
      "\n",
      "[SPLIT SUMMARY]\n",
      "Train: 1000 normal samples\n",
      "Val:   300 normal samples\n",
      "Test:  900 samples (700N + 200F)\n"
     ]
    }
   ],
   "source": [
    "# training config\n",
    "CONFIG_QUANTUM = {\n",
    "    \"n_qubits\": 12,\n",
    "    \"n_layers\": 3,\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 0.01,  # single LR for both quantum/classical (TorchLayer should handle both)\n",
    "    \"epochs\": 5,\n",
    "    \"data_dir\": \"../data/processed\",\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUANTUM TRAINING CONFIG\")\n",
    "print(\"=\"*60)\n",
    "for k, v in CONFIG_QUANTUM.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "train_loader_q, val_loader_q, test_loader_q = create_dataloaders(\n",
    "    data_dir=CONFIG_QUANTUM[\"data_dir\"],\n",
    "    batch_size=CONFIG_QUANTUM[\"batch_size\"],\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"\\n[SPLIT SUMMARY]\")\n",
    "print(f\"Train: {len(train_loader_q.dataset)} normal samples\")\n",
    "print(f\"Val:   {len(val_loader_q.dataset)} normal samples\")\n",
    "print(f\"Test:  {len(test_loader_q.dataset)} samples (700N + 200F)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eaca7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum parameters: 108\n",
      "Classical parameters: 944\n",
      "Total: 1052\n",
      "\n",
      "Epoch [1/5]\n",
      "  Batch [0/63] | Loss: 0.184363 | Time: 3.7s\n",
      "  Batch [5/63] | Loss: 0.134433 | Time: 12.1s\n",
      "  Batch [10/63] | Loss: 0.076656 | Time: 20.2s\n",
      "  Batch [15/63] | Loss: 0.026390 | Time: 28.4s\n",
      "  Batch [20/63] | Loss: 0.006138 | Time: 36.8s\n",
      "  Batch [25/63] | Loss: 0.002274 | Time: 45.0s\n",
      "  Batch [30/63] | Loss: 0.001164 | Time: 53.1s\n",
      "  Batch [35/63] | Loss: 0.000942 | Time: 61.2s\n",
      "  Batch [40/63] | Loss: 0.000083 | Time: 69.4s\n",
      "  Batch [45/63] | Loss: 0.000491 | Time: 77.5s\n",
      "  Batch [50/63] | Loss: 0.000115 | Time: 85.6s\n",
      "  Batch [55/63] | Loss: 0.000198 | Time: 93.7s\n",
      "  Batch [60/63] | Loss: 0.000139 | Time: 101.9s\n",
      "  → Train: 0.028224\n",
      "  → Val:   0.000089\n",
      "  ✓ Saved best model\n",
      "\n",
      "Epoch [2/5]\n",
      "  Batch [0/63] | Loss: 0.000089 | Time: 125.1s\n",
      "  Batch [5/63] | Loss: 0.000107 | Time: 133.0s\n",
      "  Batch [10/63] | Loss: 0.000081 | Time: 141.0s\n",
      "  Batch [15/63] | Loss: 0.000074 | Time: 148.9s\n",
      "  Batch [20/63] | Loss: 0.000067 | Time: 156.9s\n",
      "  Batch [25/63] | Loss: 0.000058 | Time: 164.9s\n",
      "  Batch [30/63] | Loss: 0.000053 | Time: 173.0s\n",
      "  Batch [35/63] | Loss: 0.000048 | Time: 181.1s\n",
      "  Batch [40/63] | Loss: 0.000043 | Time: 189.0s\n",
      "  Batch [45/63] | Loss: 0.000039 | Time: 197.0s\n",
      "  Batch [50/63] | Loss: 0.000036 | Time: 205.1s\n",
      "  Batch [55/63] | Loss: 0.000033 | Time: 213.0s\n",
      "  Batch [60/63] | Loss: 0.000031 | Time: 220.9s\n",
      "  → Train: 0.000057\n",
      "  → Val:   0.000030\n",
      "  ✓ Saved best model\n",
      "\n",
      "Epoch [3/5]\n",
      "  Batch [0/63] | Loss: 0.000030 | Time: 243.6s\n",
      "  Batch [5/63] | Loss: 0.000029 | Time: 251.6s\n",
      "  Batch [10/63] | Loss: 0.000028 | Time: 261.0s\n",
      "  Batch [15/63] | Loss: 0.000026 | Time: 270.8s\n",
      "  Batch [20/63] | Loss: 0.000025 | Time: 280.4s\n",
      "  Batch [25/63] | Loss: 0.000025 | Time: 289.8s\n",
      "  Batch [30/63] | Loss: 0.000023 | Time: 299.2s\n",
      "  Batch [35/63] | Loss: 0.000023 | Time: 308.9s\n",
      "  Batch [40/63] | Loss: 0.000022 | Time: 318.3s\n",
      "  Batch [45/63] | Loss: 0.000021 | Time: 327.5s\n",
      "  Batch [50/63] | Loss: 0.000020 | Time: 336.7s\n",
      "  Batch [55/63] | Loss: 0.000019 | Time: 346.0s\n",
      "  Batch [60/63] | Loss: 0.000018 | Time: 355.2s\n",
      "  → Train: 0.000023\n",
      "  → Val:   0.000018\n",
      "  ✓ Saved best model\n",
      "\n",
      "Epoch [4/5]\n",
      "  Batch [0/63] | Loss: 0.000018 | Time: 382.0s\n",
      "  Batch [5/63] | Loss: 0.000017 | Time: 391.2s\n",
      "  Batch [10/63] | Loss: 0.000016 | Time: 400.3s\n",
      "  Batch [15/63] | Loss: 0.000015 | Time: 409.7s\n",
      "  Batch [20/63] | Loss: 0.000015 | Time: 418.8s\n",
      "  Batch [25/63] | Loss: 0.000014 | Time: 428.0s\n",
      "  Batch [30/63] | Loss: 0.000013 | Time: 437.1s\n",
      "  Batch [35/63] | Loss: 0.000013 | Time: 446.2s\n",
      "  Batch [40/63] | Loss: 0.000012 | Time: 455.3s\n",
      "  Batch [45/63] | Loss: 0.000012 | Time: 464.4s\n",
      "  Batch [50/63] | Loss: 0.000011 | Time: 473.5s\n",
      "  Batch [55/63] | Loss: 0.000010 | Time: 482.6s\n",
      "  Batch [60/63] | Loss: 0.000010 | Time: 492.0s\n",
      "  → Train: 0.000013\n",
      "  → Val:   0.000010\n",
      "  ✓ Saved best model\n",
      "\n",
      "Epoch [5/5]\n",
      "  Batch [0/63] | Loss: 0.000010 | Time: 518.8s\n",
      "  Batch [5/63] | Loss: 0.000009 | Time: 528.0s\n",
      "  Batch [10/63] | Loss: 0.000009 | Time: 537.2s\n",
      "  Batch [15/63] | Loss: 0.000008 | Time: 546.4s\n",
      "  Batch [20/63] | Loss: 0.000008 | Time: 555.6s\n",
      "  Batch [25/63] | Loss: 0.000008 | Time: 564.6s\n",
      "  Batch [30/63] | Loss: 0.000007 | Time: 573.7s\n",
      "  Batch [35/63] | Loss: 0.000007 | Time: 582.9s\n",
      "  Batch [40/63] | Loss: 0.000007 | Time: 592.0s\n",
      "  Batch [45/63] | Loss: 0.000006 | Time: 601.4s\n",
      "  Batch [50/63] | Loss: 0.000006 | Time: 610.6s\n",
      "  Batch [55/63] | Loss: 0.000006 | Time: 619.8s\n",
      "  Batch [60/63] | Loss: 0.000006 | Time: 628.9s\n",
      "  → Train: 0.000007\n",
      "  → Val:   0.000005\n",
      "  ✓ Saved best model\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "Best val loss: 0.000005\n"
     ]
    }
   ],
   "source": [
    "# training loop (TorchLayer should handle everything lol)\n",
    "import time\n",
    "\n",
    "model_q = QuantumAutoencoder()\n",
    "optimizer = torch.optim.Adam(model_q.parameters(), lr=CONFIG_QUANTUM[\"lr\"])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_losses_q = []\n",
    "val_losses_q = []\n",
    "best_val_loss_q = float('inf')\n",
    "patience = 10\n",
    "patience_counter_q = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(CONFIG_QUANTUM['epochs']):\n",
    "    print(f\"\\nEpoch [{epoch+1}/{CONFIG_QUANTUM['epochs']}]\")\n",
    "    \n",
    "    # ===== TRAINING =====\n",
    "    model_q.train()\n",
    "    epoch_train_losses = []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader_q):\n",
    "        normal_mask = (labels == 0)\n",
    "        normal_images = images[normal_mask]\n",
    "        \n",
    "        if normal_images.shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        # forward pass (gradients should flow through quantum + classical automatically)\n",
    "        reconstructed = model_q(normal_images)\n",
    "        loss = criterion(reconstructed, normal_images)\n",
    "        \n",
    "        # backward pass (TorchLayer should handle quantum gradients)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_losses.append(loss.item())\n",
    "        \n",
    "        if batch_idx % 5 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"  Batch [{batch_idx}/{len(train_loader_q)}] | Loss: {loss.item():.6f} | Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    avg_train_loss = np.mean(epoch_train_losses)\n",
    "    train_losses_q.append(avg_train_loss)\n",
    "    print(f\"  → Train: {avg_train_loss:.6f}\")\n",
    "    \n",
    "    # ===== VALIDATION =====\n",
    "    model_q.eval()\n",
    "    epoch_val_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_q:\n",
    "            normal_mask = (labels == 0)\n",
    "            normal_images = images[normal_mask]\n",
    "            \n",
    "            if normal_images.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            reconstructed = model_q(normal_images)\n",
    "            loss = criterion(reconstructed, normal_images)\n",
    "            epoch_val_losses.append(loss.item())\n",
    "    \n",
    "    avg_val_loss = np.mean(epoch_val_losses)\n",
    "    val_losses_q.append(avg_val_loss)\n",
    "    print(f\"  → Val:   {avg_val_loss:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss_q - 1e-6:\n",
    "        best_val_loss_q = avg_val_loss\n",
    "        patience_counter_q = 0\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state': model_q.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "        }, \"../models/quantum_ae_best.pt\")\n",
    "        print(f\"  ✓ Saved best model\")\n",
    "    else:\n",
    "        patience_counter_q += 1\n",
    "        print(f\"  → No improvement for {patience_counter_q} epochs\")\n",
    "        \n",
    "        if patience_counter_q >= patience:\n",
    "            print(f\"\\n[EARLY STOPPING] Triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best val loss: {best_val_loss_q:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03a58dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[M87FlareDataset] Loaded 1000 train samples\n",
      "[M87FlareDataset] Flare count: 0\n",
      "[M87FlareDataset] Loaded 300 val samples\n",
      "[M87FlareDataset] Flare count: 0\n",
      "[M87FlareDataset] Loaded 900 test samples\n",
      "[M87FlareDataset] Flare count: 200\n",
      "\n",
      "[SPLIT SUMMARY]\n",
      "Train: 1000 samples (normal only)\n",
      "Val:   300 samples (normal only)\n",
      "Test:  900 samples (normal + flare)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "QUANTUM ROC AUC (Epoch 10): 0.9914\n",
      "CLASSICAL ROC AUC: 0.9777\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# visualize reconstructions\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "test_loader_quick, _, _ = create_dataloaders(data_dir=\"../data/processed\", batch_size=16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    images, labels = next(iter(test_loader_quick))\n",
    "    reconstructions = model_q(images[:8])\n",
    "    \n",
    "    for i in range(8):\n",
    "        axes[0, i].imshow(images[i, 0], cmap='hot', vmin=0, vmax=1)\n",
    "        axes[1, i].imshow(reconstructions[i, 0], cmap='hot', vmin=0, vmax=1)\n",
    "        axes[0, i].set_title(f\"Original (label={labels[i].item()})\")\n",
    "        axes[1, i].set_title(\"Reconstructed\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# compute ROC AUC\n",
    "all_errors_q = []\n",
    "all_labels_q = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_q:\n",
    "        errors = model_q.get_reconstruction_error(images)\n",
    "        all_errors_q.extend(errors.numpy())\n",
    "        all_labels_q.extend(labels.numpy())\n",
    "\n",
    "roc_auc_q = roc_auc_score(all_labels_q, all_errors_q)\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"QUANTUM ROC AUC (Epoch 10): {roc_auc_q:.4f}\")\n",
    "print(f\"CLASSICAL ROC AUC: 0.9777\")\n",
    "print(f\"{'='*40}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
