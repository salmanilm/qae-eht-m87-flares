{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa68bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source directory: ..\\data\\processed\\norm\n",
      "Target patches directory: ..\\data\\processed\\patches\n",
      "Target splits directory: ..\\data\\processed\\splits\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# defining paths\n",
    "data_dir = Path(\"../data/processed\")\n",
    "norm_dir = data_dir / \"norm\"\n",
    "patches_dir = data_dir / \"patches\"\n",
    "splits_dir = data_dir / \"splits\"\n",
    "\n",
    "# Create directories\n",
    "patches_dir.mkdir(exist_ok=True)\n",
    "splits_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Source directory: {norm_dir}\")\n",
    "print(f\"Target patches directory: {patches_dir}\")\n",
    "print(f\"Target splits directory: {splits_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a152151f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SCANNING FILES\n",
      "============================================================\n",
      "Total .npy files found: 2300\n",
      "Normal files: 2000\n",
      "Flare files: 300\n"
     ]
    }
   ],
   "source": [
    "# scanning and categorizing files\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCANNING FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_files = list(norm_dir.glob(\"*.npy\"))\n",
    "print(f\"Total .npy files found: {len(all_files)}\")\n",
    "\n",
    "# separate normal vs flare\n",
    "normal_files = [f for f in all_files if f.name.startswith(\"normal_\")]\n",
    "flare_files = [f for f in all_files if not f.name.startswith(\"normal_\")]\n",
    "\n",
    "print(f\"Normal files: {len(normal_files)}\")\n",
    "print(f\"Flare files: {len(flare_files)}\")\n",
    "\n",
    "# verify counts match expected ones\n",
    "if len(normal_files) != 2000:\n",
    "    print(f\"[WARNING] Expected 2000 normal files, found {len(normal_files)}\")\n",
    "if len(flare_files) != 300:\n",
    "    print(f\"[WARNING] Expected 300 flare files, found {len(flare_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406f38c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "REORGANIZING FILES\n",
      "============================================================\n",
      "Copied 2300 files to ..\\data\\processed\\patches\n",
      "Saved filename mapping to ..\\data\\processed\\filename_mapping.json\n"
     ]
    }
   ],
   "source": [
    "# rename and move\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REORGANIZING FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# new naming scheme: patch_XXXX.npy (sequential)\n",
    "file_mapping = {}  # old_name -> new_name\n",
    "\n",
    "# process normal files\n",
    "for i, old_path in enumerate(normal_files):\n",
    "    new_name = f\"patch_{i:04d}.npy\"\n",
    "    new_path = patches_dir / new_name\n",
    "    shutil.copy2(old_path, new_path)  # copy instead of move for safety\n",
    "    file_mapping[old_path.name] = new_name\n",
    "\n",
    "# process flare files (continue numbering from 2000)\n",
    "for i, old_path in enumerate(flare_files):\n",
    "    new_name = f\"patch_{2000 + i:04d}.npy\"\n",
    "    new_path = patches_dir / new_name\n",
    "    shutil.copy2(old_path, new_path)\n",
    "    file_mapping[old_path.name] = new_name\n",
    "\n",
    "print(f\"Copied {len(file_mapping)} files to {patches_dir}\")\n",
    "\n",
    "# save mapping for reference\n",
    "mapping_path = data_dir / \"filename_mapping.json\"\n",
    "with open(mapping_path, \"w\") as f:\n",
    "    json.dump(file_mapping, f, indent=2)\n",
    "print(f\"Saved filename mapping to {mapping_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb42dd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING TRAIN/VAL/TEST SPLITS\n",
      "============================================================\n",
      "Normal patches: 2000\n",
      "Flare patches: 300\n",
      "\n",
      "Train: 1000 samples (all normal)\n",
      "Val: 300 samples (all normal)\n",
      "Test: 900 samples (700 normal + 200 flare)\n",
      "Saved train_files.json: 1000 samples\n",
      "Saved val_files.json: 300 samples\n",
      "Saved test_files.json: 900 samples\n"
     ]
    }
   ],
   "source": [
    "# create split files in JSON serializable format\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING TRAIN/VAL/TEST SPLITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# load all patch metadata from the renamed files in patches_dir\n",
    "all_patches = []\n",
    "for patch_file in patches_dir.glob(\"*.npy\"):\n",
    "    idx = int(patch_file.stem.split(\"_\")[1])  # extract number from patch_XXXX.npy\n",
    "    label = 0 if idx < 2000 else 1  # first 2000 = normal, rest = flare\n",
    "    \n",
    "    all_patches.append({\n",
    "        \"path\": f\"patches/{patch_file.name}\",\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "# separate into lists of dictionaries (both JSON serializable)\n",
    "normal_patches = [p for p in all_patches if p[\"label\"] == 0]\n",
    "flare_patches = [p for p in all_patches if p[\"label\"] == 1]\n",
    "\n",
    "print(f\"Normal patches: {len(normal_patches)}\")\n",
    "print(f\"Flare patches: {len(flare_patches)}\")\n",
    "\n",
    "# anomaly detection split: train/val = normal only, test = normal + flare\n",
    "train_files = normal_patches[:1000]\n",
    "val_files = normal_patches[1000:1300]\n",
    "test_normal = normal_patches[1300:]\n",
    "test_flares = flare_patches[:200]\n",
    "\n",
    "# both are lists of dicts so its safe to concatenate\n",
    "test_files = test_normal + test_flares\n",
    "\n",
    "print(f\"\\nTrain: {len(train_files)} samples (all normal)\")\n",
    "print(f\"Val: {len(val_files)} samples (all normal)\")\n",
    "print(f\"Test: {len(test_files)} samples ({len(test_normal)} normal + {len(test_flares)} flare)\")\n",
    "\n",
    "# save splits\n",
    "def save_split_json(split_name, files):\n",
    "    split_path = splits_dir / f\"{split_name}_files.json\"\n",
    "    with open(split_path, \"w\") as f:\n",
    "        json.dump(files, f, indent=2)\n",
    "    print(f\"Saved {split_name}_files.json: {len(files)} samples\")\n",
    "\n",
    "save_split_json(\"train\", train_files)\n",
    "save_split_json(\"val\", val_files)\n",
    "save_split_json(\"test\", test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5afd14a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFYING SPLITS\n",
      "============================================================\n",
      "train: 1000 total, 0 flares\n",
      "val: 300 total, 0 flares\n",
      "test: 900 total, 200 flares\n",
      "\n",
      "✓ SUCCESS: Train and Val are pure normal (correct for anomaly detection)\n"
     ]
    }
   ],
   "source": [
    "# verify Splits \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFYING SPLITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def verify_split(split_name):\n",
    "    split_file = splits_dir / f\"{split_name}_files.json\"\n",
    "    with open(split_file, \"r\") as f:\n",
    "        files = json.load(f)\n",
    "    \n",
    "    labels = [f[\"label\"] for f in files]\n",
    "    print(f\"{split_name}: {len(files)} total, {sum(labels)} flares\")\n",
    "\n",
    "verify_split(\"train\")\n",
    "verify_split(\"val\")\n",
    "verify_split(\"test\")\n",
    "\n",
    "# critical check\n",
    "train_labels = [f[\"label\"] for f in json.load(open(splits_dir / \"train_files.json\"))]\n",
    "val_labels = [f[\"label\"] for f in json.load(open(splits_dir / \"val_files.json\"))]\n",
    "\n",
    "if sum(train_labels) == 0 and sum(val_labels) == 0:\n",
    "    print(\"\\n✓ SUCCESS: Train and Val are pure normal (correct for anomaly detection)\")\n",
    "else:\n",
    "    print(\"\\n✗ FAILURE: Train/Val contain flares!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a048b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after verification, you can delete the old norm/ directory to save space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
